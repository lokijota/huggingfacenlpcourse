{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving\n",
    "\n",
    "Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name,\n",
    "# and can be used directly with any checkpoint:\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now use the tokenizer as shown in the previous section:\n",
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1249, 1981, 2225, 174, 184, 1116, 2927, 28207, 1279, 3919, 14196, 9359, 1116, 117, 9468, 184, 16388, 22692, 185, 14089, 1161, 181, 1361, 5168, 1605, 102]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"As armas e os barões assinalados, na ocidental praia lusitana\")\n",
    "\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a tokenizer is identical to saving a model:\n",
    "# tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'arm', '##as', 'e', 'o', '##s', 'bar', '##õ', '##es', 'ass', '##inal', '##ado', '##s', ',', 'na', 'o', '##cid', '##ental', 'p', '##rai', '##a', 'l', '##us', '##ita', '##na']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"As armas e os barões assinalados, na ocidental praia lusitana\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'\", 's', 'go', 'to', 'the', 'beach', 'on', 'a', 'spaces', '##hip', 'with', 'wings', 'and', 'cat', \"'\", 's', 'p', '##aws']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Let's go to the beach on a spaceship with wings and cat's paws\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2421, 112, 188, 1301, 1106, 1103, 4640, 1113, 170, 6966, 3157, 1114, 4743, 1105, 5855, 112, 188, 185, 19194]\n"
     ]
    }
   ],
   "source": [
    "# let's get this converted to numbers\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2421, 112, 188, 1301, 1106, 1103, 4640, 1113, 170, 6966, 3157, 1114, 4743, 1105, 5855, 112, 188, 185, 19194, 102]\n"
     ]
    }
   ],
   "source": [
    "# this is not done yet. we are missing the separator tokens required by the model\n",
    "# notice that it's mostly the same as above but with 101 added at the head, 102 at the end\n",
    "\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(final_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] As armas e os barões assinalados, na ocidental praia lusitana [SEP]\n"
     ]
    }
   ],
   "source": [
    "# you can use the decode() method to see the special tokens: [CLS], [SEP]\n",
    "# These all depend on the model you are using \n",
    "inputs = tokenizer(\"As armas e os barões assinalados, na ocidental praia lusitana\")\n",
    "\n",
    "print(tokenizer.decode(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>As armas e os barões assinalados, na ocidental praia lusitana</s>\n"
     ]
    }
   ],
   "source": [
    "# example with another model, roberta, which uses a different set of special tokens, html-like\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "inputs = tokenizer(\"As armas e os barões assinalados, na ocidental praia lusitana\")\n",
    "print(tokenizer.decode(inputs['input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we know the different steps of the tokenizer, and we can go back to just using the tokenizer method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
